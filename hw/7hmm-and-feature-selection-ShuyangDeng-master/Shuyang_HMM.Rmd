---
title: "Markov Models"
author: "Shuyang"
date: "Fall 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Lab Section

In this lab, we will go over Hidden Markov Models. 

```{r, include=FALSE}
library(Biostrings)
library(HMM)
install.packages('depmixS4')
install.packages('quantmod')
library('depmixS4')
library('quantmod')
set.seed(1)
```




```{r}
Nk_lower <- 50
Nk_upper <- 150
bull_mean <- 0.1
bull_var <- 0.1
bear_mean <- -0.05
bear_var <- 0.2
```

```{r}
days <- replicate(5, sample(Nk_lower:Nk_upper, 1))
```


We can ask 3 questions using HMM. What is the most likely sequence of states(exon, intron, etc) given the observations(codons)? (Viterbi Algorithm) 

What is the probability of the observed sequence(codons)?(Forward Algorithm) 

How can we learn the HMM's parameters A and B given some data? (Forward-Backward Algorithm)


We will focus on the first question now: What is the most likely sequence of states given the observed sequence. 

We solve this by finding the maximum likelihood state for the observed sequence. The long way to do this is for every codon position, we find the probability of each state occuring, and keeping the state with the maximum probability. But this can result in near infinite number of calculations, instead, we use the viterbi algorithm.


Given this DNA sequence, what is the the most probable sequence of states?
```{r}
market_bull_1 <- rnorm( days[1], bull_mean, bull_var )
market_bear_2 <- rnorm( days[2], bear_mean, bear_var )
market_bull_3 <- rnorm( days[3], bull_mean, bull_var )
market_bear_4 <- rnorm( days[4], bear_mean, bear_var )
market_bull_5 <- rnorm( days[5], bull_mean, bull_var )
```


Initialize HMM
```{r}
true_regimes <- c( rep(1,days[1]), rep(2,days[2]), rep(1,days[3]), rep(2,days[4]), rep(1,days[5]))
returns <- c( market_bull_1, market_bear_2, market_bull_3, market_bear_4, market_bull_5)

plot(returns, type="l", xlab='', ylab="Returns")
```
```{r}
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 2, data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
```

```{r}
post_probs <- posterior(hmmfit)
layout(1:2)
plot(post_probs$state, type='s', main='True Regimes', xlab='', ylab='Regime')
matplot(post_probs[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
legend(x='topright', c('Bull','Bear'), fill=1:2, bty='n')
```

```{r}
getSymbols( "^GSPC", from="2004-01-01" )
gspcRets = diff( log( Cl( GSPC ) ) )
returns = as.numeric(gspcRets)
```

```{r}
plot(gspcRets)
```
```{r}
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 2, data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
post_probs <- posterior(hmmfit)

# Plot the returns stream and the posterior
# probabilities of the separate regimes
layout(1:2)
plot(returns, type='l', main='Regime Detection', xlab='', ylab='Returns')
matplot(post_probs[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
legend(x='bottomleft', c('Regime #1','Regime #2'), fill=1:2, bty='n')
```

```{r}
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 3, data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
post_probs <- posterior(hmmfit)

# Plot the returns stream and the posterior
# probabilities of the separate regimes
layout(1:2)
plot(returns, type='l', main='Regime Detection', xlab='', ylab='Returns')
matplot(post_probs[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
legend(x='bottomleft', c('Regime #1','Regime #2', 'Regime #3'), fill=1:3, bty='n')
```

```{r}
plot(gspcRets)
```

```{r}
plot(gspcRets)
```

```{r}
plot(gspcRets)
```
Homework

1. Create a Hidden Markov model of any system. Explain the Transition matrix and emission matrix. Evaluate your HMM  -- how well did it predict the sequence of events in your ground truth case?



